{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHy7GQlOk3gy9o5yZ0qEEU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setting"],"metadata":{"id":"PBBlqYwbjzUF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4t88TcYHTPXN"},"outputs":[],"source":["# dataset setting\n","train_path = '/kaggle/input/llm-classification-finetuning/train.csv'\n","test_path = '/kaggle/input/llm-classification-finetuning/test.csv'\n","\n","import pandas as pd\n","pd.set_option('display.unicode.escape', False)\n","train_df = pd.read_csv(train_path)\n","test_df = pd.read_csv(test_path)\n","\n","print(\"Train dataset size:\", train_df.shape)\n","print(\"Test dataset size:\", test_df.shape)"]},{"cell_type":"code","source":["# imports\n","import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import log_loss"],"metadata":{"id":"grT71d1ij_Ae","executionInfo":{"status":"aborted","timestamp":1761736239586,"user_tz":-540,"elapsed":2,"user":{"displayName":"Temple","userId":"17131389267092927185"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED = 42\n","\n","from sklearn.model_selection import StratifiedKFold\n","skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"],"metadata":{"id":"uuRnTJXIbSTL","executionInfo":{"status":"aborted","timestamp":1761736239589,"user_tz":-540,"elapsed":1092,"user":{"displayName":"Temple","userId":"17131389267092927185"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 1. Baseline Model"],"metadata":{"id":"5-cyCMnEjvOk"}},{"cell_type":"code","source":["# copy dataframe\n","step1_train_df = train_df.copy()\n","step1_test_df = test_df.copy()\n","\n","# create target variable 'y'\n","target_cols = ['winner_model_a', 'winner_model_b', 'winner_tie']\n","y_one_hot = step1_train_df[target_cols].values\n","y = np.argmax(y_one_hot, axis=1)"],"metadata":{"id":"C9jbYN71po0H","executionInfo":{"status":"aborted","timestamp":1761736239596,"user_tz":-540,"elapsed":1098,"user":{"displayName":"Temple","userId":"17131389267092927185"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# extract lexical/length features\n","def extract_length_features(df):\n","    features = pd.DataFrame()\n","    # 1. prompt length\n","    features['prompt_len'] = df['prompt'].apply(len)\n","\n","    # 2. response_a/b length\n","    features['response_a_len'] = df['response_a'].apply(len)\n","    features['response_b_len'] = df['response_b'].apply(len)\n","\n","    # 3. difference and ratio between response_a/b\n","    features['response_diff_len'] = features['response_a_len'] - features['response_b_len']\n","    features['response_ratio_len'] = features['response_a_len'] / (features['response_b_len'] + 1e-6) # prevent divided by zero\n","\n","    return features"],"metadata":{"id":"zYEk7PuIr_L6","executionInfo":{"status":"aborted","timestamp":1761736239634,"user_tz":-540,"elapsed":1134,"user":{"displayName":"Temple","userId":"17131389267092927185"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract features for training and testing datasets\n","X_train_features = extract_length_features(step1_train_df)\n","X_test_features = extract_length_features(step1_test_df)\n","\n","# Convert features to numpy array for easier indexing\n","X = X_train_features.values\n","X_test = X_test_features.values\n","y_array = y\n","\n","# Create train data result array\n","oof_preds_train = np.zeros((X.shape[0], len(target_cols)))\n","# Create test data result array (final result)\n","oof_predictions_test = np.zeros((X_test.shape[0], len(target_cols)))\n","\n","# K-Fold loop\n","for fold, (train_index, val_index) in enumerate(skf.split(X, y_array)):\n","\n","    X_train, X_val = X[train_index], X[val_index]\n","    y_train, y_val = y_array[train_index], y_array[val_index]\n","\n","    # Initialize and train the Logistic Regression model\n","    model = LogisticRegression(\n","        random_state=42,\n","        max_iter=1000\n","    )\n","\n","    # Train the model\n","    model.fit(X_train, y_train)\n","\n","    # save train data result\n","    val_predictions = model.predict_proba(X_val)\n","    oof_preds_train[val_index] = val_predictions\n","\n","    # save test data result\n","    test_fold_predictions = model.predict_proba(X_test)\n","    oof_predictions_test += test_fold_predictions\n","\n","overall_logloss = log_loss(y_array, oof_preds_train, labels=range(len(target_cols)))\n","\n","print(f\"Overall OOF Log Loss: {overall_logloss:.6f}\")\n","\n","# Save the average of test data result array\n","predictions = oof_predictions_test / skf.n_splits"],"metadata":{"id":"XmTTkUTjtP6B","executionInfo":{"status":"aborted","timestamp":1761736239637,"user_tz":-540,"elapsed":1129,"user":{"displayName":"Temple","userId":"17131389267092927185"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the submission DataFrame\n","sub_step1 = pd.DataFrame({\n","    'id': step1_test_df['id'],\n","    'winner_model_a': predictions[:, 0], # Probability for model_a\n","    'winner_model_b': predictions[:, 1], # Probability for model_b\n","    'winner_tie': predictions[:, 2]      # Probability for tie\n","})\n","\n","# Save the submission file\n","sub_step1.to_csv('submission.csv', index=False)\n","\n","print(\"'submission.csv' file created successfully.\")\n","print(sub_step1.head())"],"metadata":{"id":"Rsh9kjmovabj","executionInfo":{"status":"aborted","timestamp":1761736239640,"user_tz":-540,"elapsed":1128,"user":{"displayName":"Temple","userId":"17131389267092927185"}}},"execution_count":null,"outputs":[]}]}